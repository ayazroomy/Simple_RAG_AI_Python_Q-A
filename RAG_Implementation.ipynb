{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b776a88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Setting Up the Libraries\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "import os\n",
    "from dotenv import load_dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5327d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Vector store created and persisted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ayaz\\AppData\\Local\\Temp\\ipykernel_18296\\2149718.py:31: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectordb.persist()\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Set your OpenAI API Key\n",
    "\n",
    "load_dotenv()\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPEN_API_KEY')\n",
    "\n",
    "# Step 2: Load PDF\n",
    "pdf_path = \"./python_source.pdf\"\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "documents = loader.load()\n",
    "\n",
    "# Step 3: Split into chunks (with overlap for better context)\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=100,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \",\", \" \", \"\"]\n",
    ")\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "# Step 4: Create Embeddings\n",
    "embedding = OpenAIEmbeddings()\n",
    "\n",
    "# Step 5: Store in ChromaDB\n",
    "persist_directory = \"chroma_db_rag\"\n",
    "\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embedding,\n",
    "    persist_directory=persist_directory\n",
    ")\n",
    "\n",
    "vectordb.persist()\n",
    "print(\"âœ… Vector store created and persisted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4506a56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Load the vector store\n",
    "vectordb = Chroma(persist_directory=persist_directory, embedding_function=embedding)\n",
    "\n",
    "# Set up retriever + LLM\n",
    "retriever = vectordb.as_retriever()\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "# Define your custom prompt template\n",
    "custom_prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=\"\"\"\n",
    "You are an AI assistant helping users provide answers based on the Context only.\n",
    "ONLY use the context provided below. DO NOT use any external knowledge from outside world.\n",
    "\n",
    "Focus specifically on giving 'Answer' based on the context if available.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# # Combine into RAG chain\n",
    "# rag_chain = RetrievalQA.from_chain_type(\n",
    "#     llm=llm,\n",
    "#     retriever=retriever\n",
    "# )\n",
    "\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",  # or \"map_reduce\" for large docs\n",
    "    retriever=retriever,  # Your ChromaDB retriever\n",
    "    chain_type_kwargs={\"prompt\": custom_prompt}\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e83cab63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”Ž Answer: http://www.pyvideo.org\n"
     ]
    }
   ],
   "source": [
    "# Example query\n",
    "query = \"what is the links to Python-related videos from conferences and user-group meetings?\"\n",
    "answer = qa_chain.run(query)\n",
    "print(\"ðŸ”Ž Answer:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8972bed2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f0cba3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
